import argparse
import datetime
import logging
import math
import os
import pickle
import random
import subprocess
import time
from copy import deepcopy
from pathlib import Path
from threading import Thread

import numpy as np
import torch.distributed as dist
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
import torch.utils.data
import yaml
from torch.cuda import amp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.tensorboard import SummaryWriter
import torch
import yaml
from tensorboardX import SummaryWriter
from tqdm import tqdm

from models.yolo import Model
from ç‰©ä½“æ£€æµ‹.YOLO_LEARN import test
from ç‰©ä½“æ£€æµ‹.YOLO_LEARN.experimental import attempt_load
from ç‰©ä½“æ£€æµ‹.YOLO_LEARN.utils.autoanchor import check_anchors
from ç‰©ä½“æ£€æµ‹.YOLO_LEARN.utils.datasets import create_dataloader
from ç‰©ä½“æ£€æµ‹.YOLO_LEARN.utils.general import set_logging, check_file, increment_path, colorstr, init_seed, \
    check_dataset, one_cycle, check_img_size, labels_to_class_weights, labels_to_image_weights, strip_optimizer
from ç‰©ä½“æ£€æµ‹.YOLO_LEARN.utils.loss import ComputeLossOTA, ComputeLoss
from ç‰©ä½“æ£€æµ‹.YOLO_LEARN.utils.metrics import fitness
from ç‰©ä½“æ£€æµ‹.YOLO_LEARN.utils.plots import plot_images, plot_results
from ç‰©ä½“æ£€æµ‹.YOLO_LEARN.utils.torch_utils.utils import select_device, torch_distributed_zero_first, intersect_dicts, \
    ModelEMA, is_parallel
from ç‰©ä½“æ£€æµ‹.YOLO_LEARN.utils.wandb_logging.wandb_utils import check_wandb_resume,WandbLogger
os.environ["WANDB_API_KEY"] = "4b41bf5fa66650642626b5b93501482551768eb0"
logger = logging.getLogger(__name__)
def train(hyp, opt, device, tb_writer=None):
    logger.info(colorstr('hyperparameters: ') + ', '.join(f'{k}={v}' for k, v in hyp.items()))
    print(opt.save_dir)
    save_dir,epochs,batch_size,total_batch_size,weights,rank,freeze=Path(opt.save_dir),opt.epochs, opt.batch_size, opt.total_batch_size, opt.weights, opt.global_rank, opt.freeze

    # Directories
    wdir=save_dir/'weights'
    wdir.mkdir(parents=True,exist_ok=True)
    last=wdir/'last.pt'
    best=wdir/'best.pt'
    results_file=save_dir/'result.txt'
    # Save run settings
    with open(save_dir/'hyp.yaml','w' ) as f:yaml.dump(hyp,f,sort_keys=False)
    with open(save_dir/'opt.yaml','w') as f:yaml.dump(opt,f,sort_keys=False)
    # Configure
    plots=not opt.evolve
    cuda=device.type!='cpu'
    '''éšæœºç§å­'''
    init_seed(2+rank)
    with   open(opt.data) as f:data_dict=yaml.load(f,Loader=yaml.SafeLoader)
    is_coco = opt.data.endswith('coco.yaml')


    # '''wandb'''
    loggers={'wandb':None}
    if rank in [-1,0]:
        opt.hyp=hyp
        run_id=None
        wandb_logger=WandbLogger(opt, Path(opt.save_dir).stem, run_id, data_dict)
        loggers['wandb']=wandb_logger.wandb
        data_dict=wandb_logger.data_dict
        if wandb_logger.wandb:  weights,epochs,hyp=opt.weights,opt.epochs,opt.hyp
    '''è·å–æ ‡ç­¾'''
    nc=int(data_dict['nc'])##æ ‡ç­¾ä¸ªæ•°
    names =data_dict['names']  # åˆ†ç±»çš„ç§ç±»

    print(data_dict)


    '''models'''
    pretrained = weights.endswith('.pt')
    if pretrained:
        ckpt = torch.load(weights, map_location=device,weights_only=False)##åŠ è½½è®­ç»ƒæƒé‡æ–‡ä»¶
        model = Model(opt.cfg or ckpt['model'].yaml, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)##æ ¹æ®é…ç½®æ–‡ä»¶åˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹
        exclude=['anchor']  if (opt.cfg or hyp.get('anchors'))  and not opt.resume  else [] #å¦‚æœæŒ‡å®šäº† opt.cfg æˆ–è®¾ç½®äº†è‡ªå®šä¹‰çš„ anchorsï¼Œå¹¶ä¸”ä¸æ˜¯ç»§ç»­è®­,å°±ä¸åŠ è½½é”šæ¡†ç›¸å…³çš„æƒé‡ã€‚
        state_dict=ckpt['model'].float().state_dict()#è·å–é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºæµ®ç‚¹æ ¼å¼
        '''
        å°†é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡å’Œå½“å‰æ–°æ¨¡å‹çš„æƒé‡è¿›è¡ŒåŒ¹é…ï¼Œåªä¿ç•™å¯ä»¥å¤ç”¨çš„éƒ¨åˆ†ã€‚
        å‡½æ•°åŠŸèƒ½ï¼š
        åªä¿ç•™åç§°åŒ¹é…ä¸”å½¢çŠ¶ä¸€è‡´çš„å±‚æƒé‡ã€‚
        æ’é™¤æ‰åœ¨ exclude ä¸­æŒ‡å®šçš„å±‚ï¼ˆå¦‚é”šæ¡†ç›¸å…³æƒé‡ï¼‰ã€‚
        å‚æ•°è¯´æ˜ï¼š
        ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯é¢„è®­ç»ƒæƒé‡å­—å…¸ã€‚
        ç¬¬äºŒä¸ªå‚æ•°æ˜¯å½“å‰æ¨¡å‹çš„æƒé‡å­—å…¸ã€‚
        excludeï¼šä¸éœ€è¦åŠ è½½æƒé‡çš„å±‚ååˆ—è¡¨ã€‚
        ç»“æœï¼šå¾—åˆ°ä¸€ä¸ªä»…åŒ…å«å¯è¿ç§»æƒé‡çš„ state_dictï¼Œç”¨äºåç»­åŠ è½½ã€‚
        '''
        state_dict=intersect_dicts(state_dict,model.state_dict(), exclude=exclude)
        model.load_state_dict(state_dict, strict=False)  # state_dictï¼šä¸Šä¸€æ­¥ç­›é€‰å‡ºçš„é¢„è®­ç»ƒæƒé‡ã€‚,strict=Falseï¼šè¡¨ç¤ºä¸è¦æ±‚å®Œå…¨åŒ¹é…ï¼Œå¿½ç•¥ä¸åŒ¹é…çš„å±‚ã€‚
    else: model = Model(opt.cfg, ch=3, nc=6, anchors=hyp.get('anchors')).to(device)  # create
    with torch_distributed_zero_first(rank):#ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„æ–¹å¼è°ƒç”¨è¿™ä¸ªç±»ï¼Œä¿è¯åœ¨è¿›å…¥ with å—æ—¶ï¼š
        check_dataset(data_dict)  # check æ£€æŸ¥å¹¶ä¸‹è½½æ•°æ®é›†ï¼Œç¡®ä¿è®­ç»ƒæ‰€éœ€æ–‡ä»¶å­˜åœ¨

    train_path = data_dict['train']
    test_path = data_dict['val']

    freeze = [f'model.{x}.' for x in
              (freeze if len(freeze) > 1 else range(freeze[0]))]  # parameter names to freeze (full or partial)
    print('freeze--:', freeze)
    for k, v in model.named_parameters():
        v.requires_grad = True  # train all layers
        if any(x in k for x in freeze):
            print('freezing %s' % k)
            v.requires_grad = False

    # Optimizer  ç´¯åŠ çš„æ¢¯åº¦
    nbs = 64  # batch
    accumulate = max(round(nbs / total_batch_size), 1)##å‡ ä¸ªbatchè¿›è¡Œä¼˜åŒ–
    hyp['weight_decay'] *= total_batch_size * accumulate / nbs   ##ä¸ºäº†ä¿æŒæ­£åˆ™åŒ–æ•ˆæœçš„ä¸€è‡´æ€§ï¼Œå½“ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ—¶ï¼Œéœ€è¦æŒ‰æ¯”ä¾‹è°ƒæ•´  .å®é™…å¤§å°/æ‰¹æ¬¡å¤§å°
    logger.info(f"Scaled weight_decay = {hyp['weight_decay']}")
    print(hyp)
    print(hyp['weight_decay'],total_batch_size*accumulate/nbs,'-----')

    '''æ— æ•°æ®ï¼Œåé¢è¦å†çœ‹çœ‹'''
    pg0, pg1, pg2 = [], [], []  # optimizer parameter groups,è®¾ç½®ä¸‰ç»„æƒé‡ï¼Œä¸åŒçš„è¡°å‡ç­–ç•¥
    for k, v in model.named_modules():
        if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):
            pg2.append(v.bias)  # biases
        if isinstance(v, nn.BatchNorm2d):
            pg0.append(v.weight)  # no decay
        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):
            pg1.append(v.weight)  # apply decay
        if hasattr(v, 'im'):
            if hasattr(v.im, 'implicit'):
                pg0.append(v.im.implicit)
            else:
                for iv in v.im:
                    pg0.append(iv.implicit)
        if hasattr(v, 'imc'):
            if hasattr(v.imc, 'implicit'):
                pg0.append(v.imc.implicit)
            else:
                for iv in v.imc:
                    pg0.append(iv.implicit)
        if hasattr(v, 'imb'):
            if hasattr(v.imb, 'implicit'):
                pg0.append(v.imb.implicit)
            else:
                for iv in v.imb:
                    pg0.append(iv.implicit)
        if hasattr(v, 'imo'):
            if hasattr(v.imo, 'implicit'):
                pg0.append(v.imo.implicit)
            else:
                for iv in v.imo:
                    pg0.append(iv.implicit)
        if hasattr(v, 'ia'):
            if hasattr(v.ia, 'implicit'):
                pg0.append(v.ia.implicit)
            else:
                for iv in v.ia:
                    pg0.append(iv.implicit)
        if hasattr(v, 'attn'):
            if hasattr(v.attn, 'logit_scale'):
                pg0.append(v.attn.logit_scale)
            if hasattr(v.attn, 'q_bias'):
                pg0.append(v.attn.q_bias)
            if hasattr(v.attn, 'v_bias'):
                pg0.append(v.attn.v_bias)
            if hasattr(v.attn, 'relative_position_bias_table'):
                pg0.append(v.attn.relative_position_bias_table)
        if hasattr(v, 'rbr_dense'):
            if hasattr(v.rbr_dense, 'weight_rbr_origin'):
                pg0.append(v.rbr_dense.weight_rbr_origin)
            if hasattr(v.rbr_dense, 'weight_rbr_avg_conv'):
                pg0.append(v.rbr_dense.weight_rbr_avg_conv)
            if hasattr(v.rbr_dense, 'weight_rbr_pfir_conv'):
                pg0.append(v.rbr_dense.weight_rbr_pfir_conv)
            if hasattr(v.rbr_dense, 'weight_rbr_1x1_kxk_idconv1'):
                pg0.append(v.rbr_dense.weight_rbr_1x1_kxk_idconv1)
            if hasattr(v.rbr_dense, 'weight_rbr_1x1_kxk_conv2'):
                pg0.append(v.rbr_dense.weight_rbr_1x1_kxk_conv2)
            if hasattr(v.rbr_dense, 'weight_rbr_gconv_dw'):
                pg0.append(v.rbr_dense.weight_rbr_gconv_dw)
            if hasattr(v.rbr_dense, 'weight_rbr_gconv_pw'):
                pg0.append(v.rbr_dense.weight_rbr_gconv_pw)
            if hasattr(v.rbr_dense, 'vector'):
                pg0.append(v.rbr_dense.vector)


    if opt.adam:
        optimizer = optim.Adam(pg0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum
    else:
        optimizer = optim.SGD(pg0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)

    optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  # add pg1 with weight_decay
    optimizer.add_param_group({'params': pg2})  # add pg2 (biases)
    logger.info('Optimizer groups: %g .bias, %g conv.weight, %g other' % (len(pg2), len(pg1), len(pg0)))
    del pg0, pg1, pg2

    # Scheduler https://arxiv.org/pdf/1812.01187.pdf
    # https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#OneCycleLR
    ###å­¦ä¹ ç‡è¡°å‡çš„æ–¹å¼
    if opt.linear_lr:
        lf = lambda x: (1 - x / (epochs - 1)) * (1.0 - hyp['lrf']) + hyp['lrf']  # linear
    else:  ##è¿™æ˜¯é»˜è®¤çš„
        lf = one_cycle(1, hyp['lrf'],
                       epochs)  # cosine 1->hyp['lrf'] One Cycleçš„å­¦ä¹ ç‡å˜åŒ–è¿‡ç¨‹æ˜¯ä»lr0=0.01å‘ˆä½™å¼¦å˜åŒ–è¡°é€€åˆ°lr0*lrf = 0.01*0.1 = 0.001
    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)
    #EMA æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼Œä¼˜åŒ–è®­ç»ƒæ¨¡å‹ï¼Œè¿›è¡Œå¹³æ»‘å¤„ç†ï¼Œæå‡ç¹åèƒ½åŠ›ï¼Œï¼ŒMA ä¼šå¯¹æ¨¡å‹å‚æ•°è¿›è¡ŒåŠ æƒå¹³å‡ï¼Œä½¿å¾—è®­ç»ƒè¿‡ç¨‹ä¸­å‚æ•°æ›´æ–°æ›´åŠ ç¨³å®šï¼Œå‡å°‘å™ªå£°å¯¹æ¨¡å‹çš„å½±å“ã€‚
    #æå‡æµ‹è¯•æ€§èƒ½ï¼šä½¿ç”¨ EMA å‚æ•°è¿›è¡Œæ¨ç†æ—¶ï¼Œé€šå¸¸å¯ä»¥è·å¾—æ¯”æœªå¹³æ»‘å‚æ•°æ›´å¥½çš„æ£€æµ‹ç²¾åº¦
    #å¢å¼ºé²æ£’æ€§ï¼šEMA èƒ½å¤Ÿç¼“è§£è®­ç»ƒè¿‡ç¨‹ä¸­å› å­¦ä¹ ç‡æ³¢åŠ¨æˆ–æ•°æ®åˆ†å¸ƒå˜åŒ–å¸¦æ¥çš„ä¸ç¨³å®šæ€§ã€‚

    ema = ModelEMA(model) if rank in [-1, 0] else None
    # Resume  ##ç»§ç»­è®­ç»ƒ
    start_epoch, best_fitness = 0, 0.0
    if pretrained:
        # ä¼˜åŒ–å™¨
        if ckpt['optimizer'] is not None:
            optimizer.load_state_dict(ckpt['optimizer'])
            best_fitness = ckpt['best_fitness']

        # EMA
        if ema and ckpt.get('ema'):
            ema.ema.load_state_dict(ckpt['ema'].float().state_dict())
            ema.updates = ckpt['updates']

        # Results
        if ckpt.get('training_results') is not None:
            results_file.write_text(ckpt['training_results'])  # write results.txt

        # Epochs
        start_epoch = ckpt['epoch'] + 1
        if opt.resume:
            assert start_epoch > 0, '%s training to %g epochs is finished, nothing to resume.' % (weights, epochs)
        if epochs < start_epoch:
            logger.info('%s has been trained for %g epochs. Fine-tuning for %g additional epochs.' %
                        (weights, ckpt['epoch'], epochs))
            epochs += ckpt['epoch']  # finetune additional epochs

        del ckpt, state_dict
    ##å›¾ç‰‡å¤§å°

    gs = max(int(model.stride.max()), 32)  # è®¡ç®—æ¨¡å‹çš„æœ€å¤§ä¸‹é‡‡æ ·æ­¥é•¿ï¼ˆgrid size8,16,32ï¼‰ï¼Œç¡®ä¿è¾“å…¥å›¾åƒçš„å¤§å°æ˜¯è¯¥æ­¥é•¿çš„å€æ•°ã€‚
    nl = model.model[-1].nl  # è·å–æ£€æµ‹å±‚çš„æ•°é‡  ##ä¸‹é‡‡æ ·ä½æ•°
    '''
    å…³é”®å˜é‡ï¼š
        opt.img_sizeï¼šç”¨æˆ·æŒ‡å®šçš„å›¾åƒå°ºå¯¸ï¼ˆä¾‹å¦‚ï¼Œé»˜è®¤å€¼ä¸º [640, 640]ï¼‰ã€‚
        check_img_size(x, gs)ï¼šä¸€ä¸ªå‡½æ•°ï¼Œç¡®ä¿ x æ˜¯ gs çš„å€æ•°ã€‚å¦‚æœä¸æ˜¯ï¼Œåˆ™å‘ä¸Šå–æ•´åˆ°æœ€æ¥è¿‘ gs çš„å€æ•°ã€‚
        imgszï¼šè®­ç»ƒæ—¶ä½¿ç”¨çš„å›¾åƒå°ºå¯¸ã€‚
        imgsz_testï¼šæµ‹è¯•æ—¶ä½¿ç”¨çš„å›¾åƒå°ºå¯¸ã€‚
    '''
    imgsz, imgsz_test = [check_img_size(x, gs) for x in opt.img_size]  # è°ƒæ•´è®­ç»ƒå’Œæµ‹è¯•æ—¶çš„å›¾åƒå°ºå¯¸ï¼Œç¡®ä¿å®ƒä»¬æ˜¯ gs çš„å€æ•°ã€‚
    # DP mode
    if cuda and rank == -1 and torch.cuda.device_count() > 1:
        model = torch.nn.DataParallel(model)
    # è®­ç»ƒæ•°æ®çš„å‚æ•°ï¼šgs:ç½‘æ ¼æ­¥é•¿ï¼ˆstrideï¼Œopt:æ‰€æœ‰è®­ç»ƒå‚æ•°,,hpy:è¶…å‚æ•°ï¼ˆå¦‚æ•°æ®å¢å¼ºã€loss æƒé‡ï¼‰,
    #augment=Trueå¯ç”¨è®­ç»ƒå¢å¼º,rect:ä¿æŒå›¾åƒåŸå§‹æ¯”ä¾‹,,rank:,åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„è¿›ç¨‹ç¼–å·,world_size:æ€»è¿›ç¨‹æ•°,works:æ•°æ®åŠ è½½çº¿ç¨‹æ•°,
    #image_weights:æ˜¯å¦ä½¿ç”¨æƒé‡æŠ½æ ·,quad:åŠ è½½ quad batchï¼ˆ4x å¤§å°ï¼‰,prefix:æ—¥å¿—å‰ç¼€
    dataloader, dataset = create_dataloader(train_path, imgsz, batch_size, gs, opt,
                                            hyp=hyp, augment=True, cache=opt.cache_images, rect=opt.rect, rank=rank,
                                            world_size=opt.world_size, workers=opt.workers,
                                            image_weights=opt.image_weights, quad=opt.quad, prefix=colorstr('train: '))

    mlc = np.concatenate(dataset.labels, 0)[:, 0].max()  # max label class  ï¼Œç±»åˆ«çš„ä¸ªæ•°ï¼Œæ‰€æœ‰å›¾åƒçš„æ ‡ç­¾æ‹¼æ¥æˆä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œå¹¶æå–æœ€å¤§ç±»åˆ« IDã€‚
    nb = len(dataloader)  # number of batches
    assert mlc < nc, 'Label class %g exceeds nc=%g in %s. Possible class labels are 0-%g' % (mlc, nc, opt.data, nc - 1)#æ–­è¨€æ£€æŸ¥æ˜¯å¦å­˜åœ¨éæ³•ç±»åˆ« IDã€‚

    if rank in [-1, 0]:#åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨ï¼ˆä»…ä¸»è¿›ç¨‹æ‰§è¡Œï¼‰
        testloader = create_dataloader(test_path, imgsz_test, batch_size * 2, gs, opt,  # testloader
                                       hyp=hyp, cache=opt.cache_images and not opt.notest, rect=True, rank=-1,
                                       world_size=opt.world_size, workers=opt.workers,
                                       pad=0.5, prefix=colorstr('val: '))[0]

        if not opt.resume:#éæ¢å¤è®­ç»ƒæ—¶çš„åˆå§‹åŒ–æ“ä½œï¼Œå¦‚æœä¸æ˜¯ç»§ç»­è®­ç»ƒï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹åˆå§‹åŒ–æ“ä½œã€‚
            labels = np.concatenate(dataset.labels, 0)#æ˜¯æ‰€æœ‰å›¾åƒçš„æ ‡ç­¾æ‹¼æ¥ç»“æœã€‚
            c = torch.tensor(labels[:, 0])  # classes#æ˜¯æ‰€æœ‰æ ‡ç­¾çš„ç±»åˆ« IDï¼Œè½¬æ¢ä¸º PyTorch å¼ é‡ã€‚
            # cf = torch.bincount(c.long(), minlength=nc) + 1.  # frequency
            # model._initialize_biases(cf.to(device))
            if plots:
                #plot_labels(labels, names, save_dir, loggers)
                if tb_writer:
                    tb_writer.add_histogram('classes', c, 0)#ç»Ÿè®¡å„ç±»åˆ«å‡ºç°çš„é¢‘ç‡ã€‚

            # Anchors
            if not opt.noautoanchor:###å¦‚æœæœªç¦ç”¨è‡ªåŠ¨é”šæ¡†æ£€æµ‹ï¼Œåˆ™è°ƒç”¨ check_anchors æ ¹æ®æ•°æ®é›†é‡æ–°è®¡ç®—é”šæ¡†ã€‚
                ''''''
                check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)
            model.half().float()  # pre-reduce anchor precision float32è½¬åŒ–ä¸ºfloat16
    if cuda and rank != -1:
        model = DDP(model, device_ids=[opt.local_rank], output_device=opt.local_rank,
                    # nn.MultiheadAttention incompatibility with DDP https://github.com/pytorch/pytorch/issues/26698
                    find_unused_parameters=any(isinstance(layer, nn.MultiheadAttention) for layer in model.modules()))
    # Model parameters
    hyp['box'] *= 3. / nl  # scale to layersè°ƒæ•´è¾¹ç•Œæ¡†æŸå¤±æƒé‡ (box)ï¼Œæ ¹æ®å±‚æ•° (nl) è¿›è¡Œç¼©æ”¾ã€‚
    hyp['cls'] *= nc / 80. * 3. / nl  # scale to classes and layersåˆ†ç±»æŸå¤±æƒé‡ (cls) æ ¹æ®ç±»åˆ«æ•° (nc) å’Œå±‚æ•° (nl) è¿›è¡Œè°ƒæ•´ã€‚
    hyp['obj'] *= (imgsz / 640) ** 2 * 3. / nl  # scale to image size and layerså¯¹è±¡æŸå¤±æƒé‡ (obj) æ ¹æ®å›¾åƒå¤§å° (imgsz) å’Œå±‚æ•° (nl) ç¼©æ”¾ã€‚
    hyp['label_smoothing'] = opt.label_smoothing#å°†æ ‡ç­¾å¹³æ»‘å€¼ä»å‘½ä»¤è¡Œå‚æ•° (opt) è®¾ç½®åˆ°è¶…å‚æ•° (hyp) ä¸­ã€‚
    model.nc = nc  # attach number of classes to modelå°†ç±»åˆ«æ•°é‡ (nc) ç»‘å®šåˆ°æ¨¡å‹ä¸­ã€‚
    model.hyp = hyp  # attach hyperparameters to modelå°†è¶…å‚æ•°å­—å…¸ (hyp) ç»‘å®šåˆ°æ¨¡å‹ä¸­ã€‚
    model.gr = 1.0  # iou loss ratio (obj_loss = 1.0 or iou)è®¾ç½® IoU æŸå¤±æ¯”ä¾‹ï¼Œé»˜è®¤ä¸º 1.0ï¼Œè¡¨ç¤ºä½¿ç”¨ IoU è®¡ç®—å¯¹è±¡æŸå¤±ã€‚
    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  # attach class weightsæ ¹æ®æ•°æ®é›†ä¸­å„ç±»åˆ«çš„åˆ†å¸ƒè®¡ç®—ç±»åˆ«æƒé‡ï¼Œå¹¶ç»‘å®šåˆ°æ¨¡å‹ä¸Šï¼Œç”¨äºç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚
    model.names = names##å°†ç±»åˆ«åç§°åˆ—è¡¨ (names) ç»‘å®šåˆ°æ¨¡å‹ä¸­ï¼Œä¾¿äºåç»­è¾“å‡ºç»“æœå¯è§†åŒ–ã€‚

    # Start training
    t0 = time.time()
    nw = max(round(hyp['warmup_epochs'] * nb), 1000)  # è®¡ç®—é¢„çƒ­ï¼ˆwarmupï¼‰é˜¶æ®µçš„è¿­ä»£æ¬¡æ•°ï¼Œæœ€å°‘ä¸º 1000 æ¬¡
    # nw = min(nw, (epochs - start_epoch) / 2 * nb)  # limit warmup to < 1/2 of training
    maps = np.zeros(nc)  # mAP per classåˆå§‹åŒ–æ¯ä¸ªç±»åˆ«çš„ mAP æ•°ç»„ï¼Œç”¨äºè®°å½•éªŒè¯ç»“æœã€‚
    results = (0, 0, 0, 0, 0, 0, 0)  # åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡ç»“æœï¼ŒåŒ…æ‹¬ Pï¼ˆç²¾ç¡®ç‡ï¼‰ã€Rï¼ˆå¬å›ç‡ï¼‰ã€mAP@0.5ã€mAP@0.5-0.95 ä»¥åŠæŸå¤±å€¼ç­‰ã€‚
    scheduler.last_epoch = start_epoch - 1  # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨çš„èµ·å§‹è½®æ¬¡ï¼Œç¡®ä¿è®­ç»ƒè¿ç»­æ€§ã€‚
    scaler = amp.GradScaler(enabled=cuda)#åˆå§‹åŒ–æ¢¯åº¦ç¼©æ”¾å™¨ï¼Œç”¨äºæ··åˆç²¾åº¦è®­ç»ƒï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚
    '''
    ä½œç”¨ï¼šåˆå§‹åŒ–ä¸€ä¸ªæ”¯æŒ OTAï¼ˆOptimal Transport Assignmentï¼‰ çš„æŸå¤±å‡½æ•°ã€‚
OTA ç®€ä»‹ï¼š OTA æ˜¯ä¸€ç§æ›´å…ˆè¿›çš„æ ‡ç­¾åˆ†é…ç­–ç•¥ï¼Œå®ƒå°†ç›®æ ‡æ£€æµ‹ä¸­çš„æ­£æ ·æœ¬é€‰æ‹©é—®é¢˜å»ºæ¨¡ä¸ºä¸€ä¸ªâ€œæœ€ä¼˜ä¼ è¾“â€é—®é¢˜ï¼Œé€šè¿‡è®¡ç®—é¢„æµ‹æ¡†ä¸çœŸå®æ¡†ä¹‹é—´çš„åŒ¹é…ä»£ä»·ï¼ŒåŠ¨æ€åœ°ä¸ºæ¯ä¸ªçœŸå®æ¡†é€‰æ‹©æœ€ä½³çš„é¢„æµ‹æ¡†ã€‚ç›¸æ¯”ä¼ ç»Ÿçš„åŸºäº IoU æˆ–é˜ˆå€¼çš„é™æ€åˆ†é…æ–¹å¼ï¼ŒOTA èƒ½å¤Ÿå®ç°æ›´ç²¾ç¡®ã€æ›´åˆç†çš„æ­£æ ·æœ¬åŒ¹é…ã€‚
é€‚ç”¨åœºæ™¯ï¼š
å¤šç”¨äºè®­ç»ƒåæœŸæˆ–é«˜è´¨é‡è®­ç»ƒé˜¶æ®µã€‚
å¯ä»¥æå‡æ¨¡å‹ç²¾åº¦ï¼Œå°¤å…¶æ˜¯å¯¹å¯†é›†ç›®æ ‡åœºæ™¯è¡¨ç°æ›´å¥½ã€‚
    '''
    compute_loss_ota = ComputeLossOTA(model)  # åˆå§‹åŒ– OTA ç‰ˆæœ¬çš„æŸå¤±è®¡ç®—å™¨ï¼Œç”¨äºä¼˜åŒ–åŒ¹é…é¢„æµ‹ä¸çœŸå®æ¡†ã€‚
    '''
    ä½œç”¨ï¼šåˆå§‹åŒ–ä¸€ä¸ª åŸºç¡€ç‰ˆæœ¬çš„æ ‡å‡†æŸå¤±å‡½æ•°ã€‚
    æ ‡å‡†æŸå¤±ç»„æˆï¼š
    åŒ…æ‹¬è¾¹ç•Œæ¡†æŸå¤±ï¼ˆbox lossï¼‰
    ç›®æ ‡ç½®ä¿¡åº¦æŸå¤±ï¼ˆobjectness lossï¼‰
    åˆ†ç±»æŸå¤±ï¼ˆclassification lossï¼‰
    æ ‡ç­¾åˆ†é…æ–¹å¼ï¼š
    ä½¿ç”¨ä¼ ç»Ÿçš„ anchor åŒ¹é…ç­–ç•¥ï¼ˆåŸºäº IoU é˜ˆå€¼ï¼‰
    å¯¹æ¯ä¸ªçœŸå®æ¡†åˆ†é…ä¸€ä¸ªæˆ–å¤šä¸ª anchor ä½œä¸ºæ­£æ ·æœ¬
    é€‚ç”¨åœºæ™¯ï¼š
    é»˜è®¤ä½¿ç”¨çš„æŸå¤±å‡½æ•°
    è®­ç»ƒåˆæœŸæˆ–å¿«é€Ÿè¿­ä»£æ—¶ä½¿ç”¨
    è®¡ç®—æ•ˆç‡é«˜
    '''

    compute_loss = ComputeLoss(model)  # åˆå§‹åŒ–åŸºç¡€ç‰ˆæœ¬çš„æŸå¤±è®¡ç®—å™¨ï¼Œç”¨äºè®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±è®¡ç®—ã€‚
    logger.info(f'Image sizes {imgsz} train, {imgsz_test} test\n'
                f'Using {dataloader.num_workers} dataloader workers\n'
                f'Logging results to {save_dir}\n'
                f'Starting training for {epochs} epochs...')

    # torch.save(model, wdir / 'init.pt')

    with open(wdir / 'init.pt', 'wb') as f:
        pickle.dump(model, f, protocol=4)


    for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------
        #è¿™æ®µä»£ç æ˜¯ YOLO æ¨¡å‹è®­ç»ƒçš„ä¸»å¾ªç¯éƒ¨åˆ†ï¼Œè´Ÿè´£è¿­ä»£æ¯ä¸€ä¸ª epoch å¹¶å¤„ç†æ¯ä¸ª batch çš„æ•°æ®åŠ è½½ã€é¢„å¤„ç†ã€å­¦ä¹ ç‡é¢„çƒ­ç­‰æ“ä½œã€‚
        model.train()

        #  1. å›¾åƒæƒé‡æ›´æ–°ï¼ˆå¯é€‰ï¼‰
        if opt.image_weights:
            '''
            åˆ™æ ¹æ®ç±»åˆ«æƒé‡å’Œå½“å‰ mAP åŠ¨æ€è°ƒæ•´è®­ç»ƒå›¾åƒçš„é‡‡æ ·æ¦‚ç‡ã€‚
ä¸»è¿›ç¨‹ï¼ˆrank 0ï¼‰è®¡ç®—æ¯ä¸ªå›¾åƒçš„æƒé‡å¹¶éšæœºé‡‡æ ·ã€‚
ä½¿ç”¨ dist.broadcast å‘å…¶ä»–è¿›ç¨‹å¹¿æ’­é‡‡æ ·ç´¢å¼•ï¼ˆé€‚ç”¨äºåˆ†å¸ƒå¼è®­ç»ƒ DDPï¼‰ã€‚
            '''
            # Generate indices
            if rank in [-1, 0]:
                cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  # class weights
                iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  # image weights
                dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  # rand weighted idx
            # Broadcast if DDP
            if rank != -1:
                indices = (torch.tensor(dataset.indices) if rank == 0 else torch.zeros(dataset.n)).int()
                dist.broadcast(indices, 0)
                if rank != 0:
                    dataset.indices = indices.cpu().numpy()

        # Update mosaic border
        # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)
        # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders

        mloss = torch.zeros(4, device=device)  #  åˆå§‹åŒ–æŸå¤±è®°å½•å™¨,mloss[0]: è¾¹ç•Œæ¡†æŸå¤±ï¼ˆbox lossï¼‰,mloss[1]: ç›®æ ‡ç½®ä¿¡åº¦æŸå¤±ï¼ˆobjectness lossï¼‰,mloss[2]: åˆ†ç±»æŸå¤±ï¼ˆclassification lossï¼‰
        if rank != -1:#ğŸ”„ 4. åˆ†å¸ƒå¼è®­ç»ƒä¸­é‡ç½®é‡‡æ ·å™¨,åœ¨ DDP æ¨¡å¼ä¸‹ï¼Œä¸ºäº†ä¿è¯ä¸åŒ GPU ä¸Šçš„é‡‡æ ·é¡ºåºä¸åŒä½†å‡åŒ€åˆ†å¸ƒï¼Œéœ€è¦åœ¨æ¯è½®å¼€å§‹æ—¶è°ƒç”¨ set_epoch(epoch)ã€‚
            dataloader.sampler.set_epoch(epoch)
        pbar = enumerate(dataloader)
        logger.info(('\n' + '%10s' * 8) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'total', 'labels', 'img_size'))
        if rank in [-1, 0]:
            pbar = tqdm(pbar, total=nb)  # progress bar
        optimizer.zero_grad()#âš™ï¸ 6. æ¸…ç©ºä¼˜åŒ–å™¨æ¢¯åº¦

        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------
            #å†…å±‚å¾ªç¯ï¼Œå¤„ç†æ¯ä¸ª batch çš„æ•°æ®ã€‚ï¼Œniæ˜¯å…¨å±€æ‰¹æ¬¡ç´¢å¼•ï¼Œnbæ˜¯æ€»æ‰¹æ¬¡æ•°ï¼Œepochæ˜¯å½“å‰è½®æ•°ï¼Œtotal_batch_sizeæ˜¯æ€»æ‰¹æ¬¡å¤§å°ã€‚
            ni = i + nb * epoch  # number integrated batches (since train start)
            imgs = imgs.to(device, non_blocking=True).float() / 255.0  # uint8 to float32, 0-255 to 0.0-1.0å°†å›¾åƒæ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPU/CPUï¼‰ï¼Œå¹¶å°†åƒç´ å€¼å½’ä¸€åŒ–åˆ° [0, 1]ã€‚

            # Warmup  é¢„çƒ­ï¼Œå­¦ä¹ ç‡
            if ni <= nw:
                '''
                ä½¿ç”¨çº¿æ€§æ’å€¼é€æ­¥å¢åŠ å­¦ä¹ ç‡ï¼ˆbias ç»„ä» warmup_bias_lr å¢åŠ åˆ°åˆå§‹ lrï¼Œå…¶ä»–ç»„ä» 0 å¢åŠ ï¼‰ã€‚
momentum ä¹Ÿéšè®­ç»ƒæ­¥æ•°çº¿æ€§å¢é•¿ã€‚
accumulate è¡¨ç¤ºå¤šå°‘ä¸ª batch åæ‰æ›´æ–°ä¸€æ¬¡å‚æ•°ï¼Œæ¨¡æ‹Ÿå¤§ batch size æ•ˆæœã€‚
ğŸ“Œ ä½œç”¨ï¼šé˜²æ­¢æ¨¡å‹åˆæœŸå› å­¦ä¹ ç‡è¿‡å¤§å¯¼è‡´ä¸ç¨³å®šï¼Œæé«˜æ”¶æ•›é€Ÿåº¦ä¸ç¨³å®šæ€§ã€‚

                '''
                xi = [0, nw]  # x interp
                # model.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)
                accumulate = max(1, np.interp(ni, xi, [1, nbs / total_batch_size]).round())
                for j, x in enumerate(optimizer.param_groups):
                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0
                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])
                    if 'momentum' in x:
                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])

            #  å¤šå°ºåº¦è®­ç»ƒï¼ˆMulti-scale Trainingï¼‰ï¼Œ
            '''
            ä½œç”¨ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºæ”¹å˜è¾“å…¥å›¾åƒçš„å°ºå¯¸ï¼Œæé«˜æ¨¡å‹å¯¹ä¸åŒå°ºåº¦ç›®æ ‡çš„é²æ£’æ€§ã€‚
imgsz æ˜¯åŸºç¡€å›¾åƒå¤§å°ï¼ˆå¦‚ 640ï¼‰ï¼Œgs æ˜¯æ¨¡å‹çš„æœ€å¤§ä¸‹é‡‡æ ·æ­¥é•¿ï¼ˆå¦‚ 32ï¼‰ã€‚
éšæœºé€‰æ‹©ä¸€ä¸ªä»‹äº imgsz * 0.5 åˆ° imgsz * 1.5 çš„å›¾åƒå¤§å°ï¼Œå¹¶ç¡®ä¿å…¶ä¸º gs çš„æ•´æ•°å€ã€‚
è®¡ç®—ç¼©æ”¾å› å­ sfï¼Œå¹¶æ ¹æ®è¯¥å› å­é‡æ–°è®¡ç®—å›¾åƒçš„æ–°å°ºå¯¸ nsï¼ˆä¿æŒä¸ gs å¯¹é½ï¼‰ã€‚
ä½¿ç”¨åŒçº¿æ€§æ’å€¼å°†å›¾åƒè°ƒæ•´åˆ°æ–°å°ºå¯¸ã€‚
            '''
            if opt.multi_scale:
                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size
                sf = sz / max(imgs.shape[2:])  # scale factor
                if sf != 1:
                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)
                    imgs = F.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)

            # Forward
            '''
            ä½¿ç”¨ amp.autocast() å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16/FP32ï¼‰ï¼Œå‡å°‘æ˜¾å­˜å ç”¨å¹¶åŠ é€Ÿè®­ç»ƒã€‚
å°†å¤„ç†åçš„å›¾åƒæ•°æ® imgs è¾“å…¥æ¨¡å‹ï¼Œå¾—åˆ°é¢„æµ‹ç»“æœ predï¼ˆé€šå¸¸æ˜¯å¤šä¸ªç‰¹å¾å±‚çš„è¾“å‡ºåˆ—è¡¨ï¼‰ã€‚
            '''
            with amp.autocast(enabled=cuda):
                pred = model(imgs)  # forward
                if hyp['loss_ota'] == 1:#ğŸ’¥ 3. æŸå¤±è®¡ç®—ï¼ˆLoss Computationï¼‰
                    print(f'---------{device}---------------')
                    loss, loss_items = compute_loss_ota(pred, targets.to(device), imgs)  #ComputeLossOTA: æ”¯æŒ OTAï¼ˆOptimal Transport Assignmentï¼‰çš„æŸå¤±å‡½æ•°ï¼Œç”¨äºæ›´ç²¾ç¡®çš„æ­£æ ·æœ¬åˆ†é…ï¼Œé€‚ç”¨äºé«˜è´¨é‡è®­ç»ƒã€‚
                else:
                    loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_sizeåŸºç¡€ç‰ˆæœ¬ï¼Œä½¿ç”¨ä¼ ç»Ÿ anchor åŒ¹é…ç­–ç•¥ï¼Œè®¡ç®—æ•ˆç‡æ›´é«˜ã€‚
                if rank != -1:#åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„æŸå¤±å¹³å‡,åœ¨ DDPï¼ˆåˆ†å¸ƒå¼è®­ç»ƒï¼‰æ¨¡å¼ä¸‹ï¼Œæ€»æŸå¤±ä¹˜ä»¥ world_sizeï¼Œä»¥ä¾¿åç»­æ¢¯åº¦å¹³å‡ã€‚
                    loss *= opt.world_size  # gradient averaged between devices in DDP mode
                if opt.quad:#å¦‚æœå¯ç”¨ --quadï¼ˆåŠ è½½å››å€å¤§å°çš„ batchï¼‰ï¼Œåˆ™æŸå¤±æ”¾å¤§ 4 å€ï¼Œæ¨¡æ‹Ÿæ›´å¤§ batch size çš„æ•ˆæœã€‚
                    loss *= 4.

            # Backward
            scaler.scale(loss).backward()#ä½¿ç”¨ GradScaler ç¼©æ”¾æŸå¤±ï¼Œé˜²æ­¢ FP16 ä¸‹çš„æ¢¯åº¦æº¢å‡ºã€‚æ‰§è¡Œåå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦ã€‚

            # Optimize
            '''
            æ¢¯åº¦ç´¯ç§¯ï¼šæ¯ accumulate æ­¥æ›´æ–°ä¸€æ¬¡æƒé‡ã€‚
scaler.step(optimizer)ï¼šæ‰§è¡Œä¼˜åŒ–å™¨æ›´æ–°ã€‚
scaler.update()ï¼šæ›´æ–°ç¼©æ”¾å› å­ã€‚
optimizer.zero_grad()ï¼šæ¸…ç©ºå½“å‰æ¢¯åº¦ã€‚
ema.update(model)ï¼šæ›´æ–° EMAï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰æ¨¡å‹ï¼Œæå‡æ¨¡å‹ç¨³å®šæ€§ä¸æ¨ç†æ€§èƒ½ã€‚
            '''
            if ni % accumulate == 0:
                scaler.step(optimizer)  # optimizer.step
                scaler.update()
                optimizer.zero_grad()
                if ema:
                    ema.update(model)

            # Print
            '''
            æ›´æ–°å½“å‰ epoch ä¸­å„æŸå¤±é¡¹çš„å¹³å‡å€¼ã€‚
æ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„æ˜¾å­˜å¤§å°ã€‚
æ„å»ºå¹¶æ˜¾ç¤ºæ—¥å¿—ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
å½“å‰ epoch / æ€» epochï¼›
æ˜¾å­˜å ç”¨ï¼›
å„é¡¹æŸå¤±ï¼›
ç›®æ ‡æ•°é‡ï¼›
å›¾åƒå°ºå¯¸ã€‚
            '''
            if rank in [-1, 0]:
                mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses
                mem = '%.3gG' % (torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0)  # (GB)
                s = ('%10s' * 2 + '%10.4g' * 6) % (
                    '%g/%g' % (epoch, epochs - 1), mem, *mloss, targets.shape[0], imgs.shape[-1])
                pbar.set_description(s)

                # Plot
                if plots and ni < 10:# å›¾åƒå¯è§†åŒ–ï¼ˆPlottingï¼‰
                    f = save_dir / f'train_batch{ni}.jpg'  # filename
                    Thread(target=plot_images, args=(imgs, targets, paths, f), daemon=True).start()
                    # if tb_writer:
                    #     tb_writer.add_image(f, result, dataformats='HWC', global_step=epoch)
                    #     tb_writer.add_graph(torch.jit.trace(model, imgs, strict=False), [])  # add model graph
                elif plots and ni == 10 and wandb_logger.wandb:
                    wandb_logger.log({"Mosaics": [wandb_logger.wandb.Image(str(x), caption=x.name) for x in
                                                  save_dir.glob('train*.jpg') if x.exists()]})

            # end batch ------------------------------------------------------------------------------------------------
        # end epoch ----------------------------------------------------------------------------------------------------

        # Scheduleræ˜¯ä¸€ä¸ªå­¦ä¹ ç‡è°ƒåº¦å™¨
        lr = [x['lr'] for x in optimizer.param_groups]  # for tensorboard
        scheduler.step()

        # DDP process 0 or single-GPU
        if rank in [-1, 0]:#ä¸»è¿›ç¨‹æˆ–å• GPU
            # mAP
            '''
            test.test(...)ï¼šè°ƒç”¨æµ‹è¯•å‡½æ•°ï¼Œè¿”å›æŒ‡æ ‡å¦‚ï¼š
results: [P, R, mAP@0.5, mAP@0.5:0.95, ...]
maps: æ¯ä¸ªç±»åˆ«çš„ mAP å€¼ï¼›
times: æ¨ç†æ—¶é—´ï¼›
model=ema.emaï¼šä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œæå‡ç¨³å®šæ€§ï¼›
imgsz_testï¼šæµ‹è¯•å›¾åƒå¤§å°ï¼›
dataloader=testloaderï¼šéªŒè¯é›†æ•°æ®åŠ è½½å™¨ï¼›
plots=Trueï¼šä¿å­˜æ··æ·†çŸ©é˜µã€PR æ›²çº¿ç­‰å¯è§†åŒ–ç»“æœï¼›
is_cocoï¼šå¦‚æœæ˜¯ COCO æ•°æ®é›†ï¼Œä¼šå¯ç”¨ç›¸åº”çš„è¯„ä¼°æ–¹å¼
            '''
            ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'gr', 'names', 'stride', 'class_weights']) # æ›´æ–° EMA æ¨¡å‹å±æ€§ï¼ˆå¦‚ç±»åˆ«æ•°ã€è¶…å‚æ•°ç­‰ï¼‰
            final_epoch = epoch + 1 == epochs# åˆ¤æ–­æ˜¯å¦ä¸ºæœ€åä¸€ä¸ª epoch
            if not opt.notest or final_epoch:  # Calculate mAP # å¦‚æœä¸æ˜¯ç¦ç”¨æµ‹è¯•æˆ–æœ€åä¸€è½®
                wandb_logger.current_epoch = epoch + 1
                results, maps, times = test.test(data_dict, # è°ƒç”¨ test.py è¿›è¡ŒéªŒè¯é›†æµ‹è¯•
                                                 batch_size=batch_size * 2,
                                                 imgsz=imgsz_test,
                                                 model=ema.ema,
                                                 single_cls=opt.single_cls,
                                                 dataloader=testloader,
                                                 save_dir=save_dir,
                                                 verbose=nc < 50 and final_epoch,
                                                 plots=plots and final_epoch,
                                                 wandb_logger=wandb_logger,
                                                 compute_loss=compute_loss,
                                                 is_coco=is_coco)

            # Write
            with open(results_file, 'a') as f:
                f.write(s + '%10.4g' * 7 % results + '\n')  # append metrics, val_loss
            if len(opt.name) and opt.bucket:
                os.system('gsutil cp %s gs://%s/results/results%s.txt' % (results_file, opt.bucket, opt.name))

            # Log
            tags = ['train/box_loss', 'train/obj_loss', 'train/cls_loss',  # train loss
                    'metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 'metrics/mAP_0.5:0.95',
                    'val/box_loss', 'val/obj_loss', 'val/cls_loss',  # val loss
                    'x/lr0', 'x/lr1', 'x/lr2']  # params
            for x, tag in zip(list(mloss[:-1]) + list(results) + lr, tags):
                if tb_writer:
                    tb_writer.add_scalar(tag, x, epoch)  # tensorboard
                if wandb_logger.wandb:
                    wandb_logger.log({tag: x})  # W&B

            # Update best mAP  æ›´æ–°æœ€ä½³æ¨¡å‹ï¼ˆBest Fitnessï¼‰
            fi = fitness(np.array(results).reshape(1, -1))  # weighted combination of [P, R, mAP@.5, mAP@.5-.95]
            if fi > best_fitness:
                best_fitness = fi
            wandb_logger.end_epoch(best_result=best_fitness == fi)

            # Save model  ğŸ’¾ å…­ã€
            if (not opt.nosave) or (final_epoch and not opt.evolve):  # if save
                ckpt = {'epoch': epoch,
                        'best_fitness': best_fitness,
                        'training_results': results_file.read_text(),
                        'model': deepcopy(model.module if is_parallel(model) else model).half(),
                        'ema': deepcopy(ema.ema).half(),
                        'updates': ema.updates,
                        'optimizer': optimizer.state_dict(),
                        'wandb_id': wandb_logger.wandb_run.id if wandb_logger.wandb else None}

                # Save last, best and delete
                with open(last, 'wb') as f:
                    pickle.dump(ckpt, f, protocol=4)
                # torch.save(ckpt, last)
                if best_fitness == fi:
                    with open(best, 'wb') as f:
                        pickle.dump(ckpt, f, protocol=4)
                    # torch.save(ckpt, best)
                if (best_fitness == fi) and (epoch >= 200):
                    # torch.save(ckpt, wdir / 'best_{:03d}.pt'.format(epoch))
                    with open( wdir / 'best_{:03d}.pt'.format(epoch), 'wb') as f:
                        pickle.dump(ckpt, f, protocol=4)

                if epoch == 0:
                    # torch.save(ckpt, wdir / 'epoch_{:03d}.pt'.format(epoch))
                    with open(wdir / 'epoch_{:03d}.pt'.format(epoch), 'wb') as f:
                        pickle.dump(ckpt, f, protocol=4)
                elif ((epoch+1) % 25) == 0:
                    # torch.save(ckpt, wdir / 'epoch_{:03d}.pt'.format(epoch))
                    with open( wdir / 'epoch_{:03d}.pt'.format(epoch), 'wb') as f:
                        pickle.dump(ckpt, f, protocol=4)
                elif epoch >= (epochs-5):
                    # torch.save(ckpt, wdir / 'epoch_{:03d}.pt'.format(epoch))
                    with open(wdir / 'epoch_{:03d}.pt'.format(epoch), 'wb') as f:
                        pickle.dump(ckpt, f, protocol=4)
                if wandb_logger.wandb:
                    if ((epoch + 1) % opt.save_period == 0 and not final_epoch) and opt.save_period != -1:
                        wandb_logger.log_model(
                            last.parent, opt, epoch, fi, best_model=best_fitness == fi)
                del ckpt

        # end epoch ----------------------------------------------------------------------------------------------------
    # end training  è®­ç»ƒç»“æŸåçš„æ“ä½œ
    '''
    plot_results(...)ï¼šç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„ lossã€precisionã€recallã€mAP æ›²çº¿ï¼›
å¦‚æœå¯ç”¨äº† WandBï¼Œåˆ™ä¸Šä¼ è¿™äº›å›¾è¡¨ã€‚
    '''
    if rank in [-1, 0]:
        # Plots
        # if plots:
        #     plot_results(save_dir=save_dir)  # save as results.png
        #     if wandb_logger.wandb:
        #         files = ['results.png', 'confusion_matrix.png', *[f'{x}_curve.png' for x in ('F1', 'PR', 'P', 'R')]]
        #         wandb_logger.log({"Results": [wandb_logger.wandb.Image(str(save_dir / f), caption=f) for f in files
        #                                       if (save_dir / f).exists()]})
        # Test best.pt
        logger.info('%g epochs completed in %.3f hours.\n' % (epoch - start_epoch + 1, (time.time() - t0) / 3600))

        #æœ€ç»ˆæµ‹è¯•ï¼ˆFinal Testï¼‰,åœ¨ COCO æ•°æ®é›†ä¸Šæ‰§è¡Œæœ€ç»ˆæµ‹è¯•ï¼›,è¾“å‡º JSON æ–‡ä»¶ç”¨äºå®˜æ–¹æäº¤ã€‚
        if opt.data.endswith('coco.yaml') and nc == 80:  # if COCO
            for m in (last, best) if best.exists() else (last):  # speed, mAP tests
                results, _, _ = test.test(opt.data,
                                          batch_size=batch_size * 2,
                                          imgsz=imgsz_test,
                                          conf_thres=0.001,
                                          iou_thres=0.7,
                                          model=attempt_load(m, device).half(),
                                          single_cls=opt.single_cls,
                                          dataloader=testloader,
                                          save_dir=save_dir,
                                          save_json=True,
                                          plots=False,
                                          is_coco=is_coco)

        # Strip optimizers åˆ é™¤ä¼˜åŒ–å™¨
        final = best if best.exists() else last  # final model
        for f in last, best:
            if f.exists():
                strip_optimizer(f)  # strip optimizers ã€æ¸…ç†ä¼˜åŒ–å™¨ï¼ˆStrip Optimizersï¼‰,å¯¹éƒ¨ç½²å’Œæ¨ç†æ›´å‹å¥½ã€‚
        if opt.bucket:
            os.system(f'gsutil cp {final} gs://{opt.bucket}/weights')  # upload ä¸Šä¼ äº‘ç«¯ï¼Œå¦‚æœè®¾ç½®äº† --bucketï¼Œåˆ™è‡ªåŠ¨ä¸Šä¼ è®­ç»ƒç»“æœæ–‡ä»¶ã€‚
        if wandb_logger.wandb and not opt.evolve:  # Log the stripped model
            wandb_logger.wandb.log_artifact(str(final), type='model',
                                            name='run_' + wandb_logger.wandb_run.id + '_model',
                                            aliases=['last', 'best', 'stripped']) #WandB æ¨¡å‹ä¸Šä¼ 
        wandb_logger.finish_run()
    else:
        dist.destroy_process_group()
    torch.cuda.empty_cache()
    return results


if __name__ == '__main__':
    parser=argparse.ArgumentParser()
    parser.add_argument('--weights',type=str,default='./yolov7.pt')
    parser.add_argument('--cfg',type=str,default='')
    parser.add_argument('--data', type=str, default='data/coco.yaml', help='data.yaml path')  ##æ•°æ®åœ°å€
    parser.add_argument('--hyp',type=str,default='data/hyp.scratch.p5.yaml')
    parser.add_argument('--epochs', type=int, default=300)  ####å¾ªç¯å‡ æ¬¡
    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs')  ###ä¸€æ¬¡æœ‰å¤šå°‘ä¸ªå›¾ç‰‡
    parser.add_argument('--img-size',nargs='+',type=int,default=[640,640])
    parser.add_argument('--rect', action='store_true')
    parser.add_argument('--resume',nargs='?',const=True,default=False,)
    parser.add_argument('--nosave',action='store_true')
    parser.add_argument('--notest',action='store_true')
    parser.add_argument('--noautoanchor', action='store_true', help='disable autoanchor check')  ##è‡ªåŠ¨æ£€æŸ¥
    parser.add_argument('--evole',action='store_true')
    parser.add_argument('--evolve', action='store_true', help='evolve hyperparameters')  ###å­¦ä¹ è¿›è¡Œè¿­ä»£
    parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')
    parser.add_argument('--cache-images', action='store_true', help='cache images for faster training')  ##å›¾åƒä¿å­˜
    parser.add_argument('--image-weights', action='store_true',
                        help='use weighted image selection for training')  ##å›¾åƒæƒé‡
    parser.add_argument('--device',default='')
    parser.add_argument('--multi-scale',action='store_true')
    parser.add_argument('--single-cls', action='store_true', help='train multi-class data as single-class')
    parser.add_argument('--adam', action='store_true', help='use torch.optim.Adam() optimizer')  ###ä¼˜åŒ–å™¨
    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')  ###è·¨å¡
    parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify')  #
    parser.add_argument('--workers', type=int, default=8, help='maximum number of dataloader workers')
    parser.add_argument('--project', default='runs/train', help='save to project/name')  ###é¡¹ç›®ä¿å­˜çš„è·¯å¾„
    parser.add_argument('--entity', default=None, help='W&B entity')  ##å¯è§†åŒ–ç›¸å…³çš„ï¼Œç”¨äºæ—¥å¿—
    parser.add_argument('--name', default='exp', help='save to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--quad', action='store_true', help='quad dataloader')
    parser.add_argument('--linear-lr', action='store_true', help='linear LR')  ##å­¦ä¹ ç‡
    parser.add_argument('--label-smoothing', type=float, default=0.0, help='Label smoothing epsilon')  ##å¹³æ»‘æ ‡ç­¾å¤„ç†
    parser.add_argument('--upload_dataset', action='store_true', help='Upload dataset as W&B artifact table')
    parser.add_argument('--bbox_interval', type=int, default=-1,
                        help='Set bounding-box image logging interval for W&B')  ##ä¹Ÿæ˜¯æ—¥å¿—ç”¨åˆ°çš„
    parser.add_argument('--save_period', type=int, default=-1,
                        help='Log model after every "save_period" epoch')  ##ä¿å­˜æœ€å¥½çš„ä¸€æ¬¡
    parser.add_argument('--artifact_alias', type=str, default="latest", help='version of dataset artifact to be used')
    parser.add_argument('--freeze', nargs='+', type=int, default=[0],
                        help='Freeze layers: backbone of yolov7=50, first3=0 1 2')  ##å†»ç»“ï¼Œä¸€èˆ¬åªå†»ç»“å‰å‡ å±‚
    opt = parser.parse_args()
    ''' 
    --workers 1
--device 0
--batch-size 4
--data  data/neu.yaml
--cfg cfg/training/yolov7.yaml
--weights yolov7.pt
--name yolov7
--hyp data/hyp.scratch.p5.yaml
--epochs 20   '''
    opt.world_size=1
    opt.global_rank =-1  ###å•ç¨‹
    set_logging(opt.global_rank)
    # wandb_run=check_wandb_resume(opt)
    # print(wandb_run,opt.resume)
    opt.data, opt.cfg, opt.hyp = check_file(opt.data), check_file(opt.cfg), check_file(opt.hyp)  # check
    opt.name = 'evolve' if opt.evolve else opt.name   ##ä¸è¿›è¡Œå­¦ä¹ çš„è¿­ä»£

    opt.save_dir = increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok | opt.evolve)  ##æ–‡ä»¶ä¿å­˜çš„è·¯å¾„
    # DDP mode # DDP mode
    opt.total_batch_size = opt.batch_size
    device=select_device(opt.device,batch_size=opt.batch_size)
    print(device)

    '''ä»¥ä¸Šæ˜¯å‡†å¤‡ï¼Œè¶…å‚æ•°æ–‡ä»¶ï¼Œæ•°æ®æ–‡ä»¶ï¼Œæ¨¡å‹æ–‡ä»¶ï¼Œä½¿ç”¨çš„è®¾å¤‡ï¼Œæ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼ï¼Œä¿å­˜çš„è·¯å¾„'''
    with open(opt.hyp) as f: hyp=yaml.load(f,Loader=yaml.SafeLoader)

    ###è®­ç»ƒæ•°æ®
    logging.info(opt)
    if not opt.evolve:  ####æ²¡æœ‰è¿›è¡Œå­¦ä¹ è¿­ä»£çš„æ¨¡å‹è®­ç»ƒ
        tb_writer = None  # init loggers
        train(hyp,opt,device,tb_writer)


